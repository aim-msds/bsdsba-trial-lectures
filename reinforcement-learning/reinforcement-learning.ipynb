{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3be32996",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-31T09:37:03.666862Z",
     "start_time": "2023-03-31T09:37:03.664107Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "!wget https://raw.githubusercontent.com/aim-msds/bsdsba-trial-lectures/main/reinforcement-learning/tictactoe.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09e28b93",
   "metadata": {},
   "source": [
    "# Introduction to Reinforcement Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "599e30d1",
   "metadata": {},
   "source": [
    "<img src='images/cover.jpeg' width=550/>\n",
    "\n",
    "In this notebook, we will demonstrate how we can train a computer to learn how to play the game *tic-tac-toe*. As discussed we will use a *temporal-difference* learning as our method to obtain the policy that the computer will use to play the game.\n",
    "\n",
    "$$\n",
    "V(S_t) \\leftarrow V(S_t) + \\alpha[V(S_{t + 1}) - V(S_t)]\n",
    "$$\n",
    "\n",
    "*Note: code has been adapted from https://github.com/JaeDukSeo/reinforcement-learning-an-introduction/blob/master/chapter01/TicTacToe.py*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a10697e",
   "metadata": {},
   "source": [
    "## Code Components"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "665783c3",
   "metadata": {},
   "source": [
    "We will first discuss the important components of the code. This involves objects that we will need to create a Tic-Tac-Toe game. And functions which we will use for training, testing, and deployment (actual playing)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a081ec9",
   "metadata": {},
   "source": [
    "### Objects"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18971c47",
   "metadata": {},
   "source": [
    "There are three objects that are core to the game of TicTacToe: `State()`, `Player()`, and `Judger()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21919f69",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-31T09:50:50.715773Z",
     "start_time": "2023-03-31T09:50:50.709125Z"
    }
   },
   "outputs": [],
   "source": [
    "from tictactoe import State, Player, Judger"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c3ea58a",
   "metadata": {},
   "source": [
    "#### `State`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6a173dc",
   "metadata": {},
   "source": [
    "The `State` object represents the current state of the Tic Tac Toe game. It consists of a 2-dimensional array that represents the board and stores the positions of X's and O's. The `State` object has methods to update the board with a player's move, check for a winner, and determine if the game has ended in a tie. It is responsible for keeping track of the game state and making sure that valid moves are made."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a835d9b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-31T09:34:45.673375Z",
     "start_time": "2023-03-31T09:34:45.666016Z"
    }
   },
   "outputs": [],
   "source": [
    "board = State()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1830d128",
   "metadata": {},
   "source": [
    "##### Empty board state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61612251",
   "metadata": {},
   "source": [
    "For example, at the start, we have an empty board state and each attribute of the board can be visualize as such:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6075fc04",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-31T09:34:47.476359Z",
     "start_time": "2023-03-31T09:34:47.465096Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "board.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f55937f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-31T09:35:21.359032Z",
     "start_time": "2023-03-31T09:35:21.350371Z"
    }
   },
   "outputs": [],
   "source": [
    "board.getHash()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2056a62f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-31T09:34:51.721112Z",
     "start_time": "2023-03-31T09:34:51.716859Z"
    }
   },
   "outputs": [],
   "source": [
    "board.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f12a0b0",
   "metadata": {},
   "source": [
    "##### Non-empty board state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23d15199",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-31T09:38:50.580293Z",
     "start_time": "2023-03-31T09:38:50.577027Z"
    }
   },
   "outputs": [],
   "source": [
    "board.hashVal = None\n",
    "board.data = np.array([\n",
    "    [0, -1, 0],\n",
    "    [1, 0, -1],\n",
    "    [1, 0, 1]\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3c90e4c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-31T09:38:51.696973Z",
     "start_time": "2023-03-31T09:38:51.692024Z"
    }
   },
   "outputs": [],
   "source": [
    "board.getHash()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7732748",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-31T09:38:52.745273Z",
     "start_time": "2023-03-31T09:38:52.741096Z"
    }
   },
   "outputs": [],
   "source": [
    "board.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92f2c9b9",
   "metadata": {},
   "source": [
    "#### `Player`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "693a8e28",
   "metadata": {},
   "source": [
    "The `Player` object represents a player in the Tic Tac Toe game. It has a symbol (either `'X'` or `'O'`) that represents the player's piece on the board. The `Player` object has a method to make a move, which involves selecting a position on the board to place the player's piece. It is responsible for deciding the best move to make given the current game state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18a7bdd7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-31T09:52:08.885169Z",
     "start_time": "2023-03-31T09:52:08.878553Z"
    }
   },
   "outputs": [],
   "source": [
    "player = Player()\n",
    "player.setSymbol(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a00c072",
   "metadata": {},
   "source": [
    "The `Player` stores the value estimation of each state in its `estimations` attribute and is being updated when the `feedReward` method has been called."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "247615a3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-31T09:52:14.792676Z",
     "start_time": "2023-03-31T09:52:14.762430Z"
    }
   },
   "outputs": [],
   "source": [
    "player.estimations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e7da033",
   "metadata": {},
   "source": [
    "#### `Judger`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3760b9fc",
   "metadata": {},
   "source": [
    "The `Judger` object is responsible for allowing the gameplay between two `Player` objects. It takes in the current `State` object and determines if there is a winner or if the game has ended in a tie. Once the game is done, it then proceeds to feed in the appropriate reward for the different players."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "157ae50c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-31T10:01:11.696909Z",
     "start_time": "2023-03-31T10:01:11.683671Z"
    }
   },
   "outputs": [],
   "source": [
    "player1 = Player()\n",
    "player2 = Player()\n",
    "judger = Judger(player1, player2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3514e669",
   "metadata": {},
   "source": [
    "Here is a sample playthrough between two players"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04460da4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-31T10:03:27.544173Z",
     "start_time": "2023-03-31T10:03:27.536308Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "winner = judger.play(show=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf5d47d3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-31T10:03:37.062089Z",
     "start_time": "2023-03-31T10:03:37.056143Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "winner"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "389c0cdb",
   "metadata": {},
   "source": [
    "We will also notice that the value estimation of each state of each player has been updated appropriately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "700af2a2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-31T10:11:35.155284Z",
     "start_time": "2023-03-31T10:11:35.150813Z"
    }
   },
   "outputs": [],
   "source": [
    "player1.estimations['[0, 0, 0, 1, 0, 0, 0, 0, 0]']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f264709",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-31T10:12:20.700793Z",
     "start_time": "2023-03-31T10:12:20.693660Z"
    }
   },
   "outputs": [],
   "source": [
    "player2.estimations['[0, 1, -1, 1, 0, 0, -1, 0, 0]']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41adfca7",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a79528ab",
   "metadata": {},
   "source": [
    "To make use of these three objects, three functions have also been defined in the code. These are `train()`, `compete()`, and `play()`. These three functions allow training of players, simulate competition between them, and allow us to play with a trained AI given its policy. Briefly, the description of these functions are:\n",
    "\n",
    "1. `train()`: The `train()` function trains the two player agents (represented by the Player class) to play Tic Tac Toe by having them play against each other for a specified number of games. During training, each player tries to learn the optimal strategy to win the game. The function updates the players' strategies based on the results of the games played, such that the players gradually learn to make better moves.\n",
    "\n",
    "2. `compete()`: The `compete()` function is used to evaluate the performance of the two player agents after they have been trained. The function has the two players play a specified number of games against each other, and keeps track of the number of wins, losses, and ties for each player. Based on the results, the function prints out the overall win rate for each player.\n",
    "\n",
    "3. `play()`: The `play()` function allows a human player to play against the trained AI player. The function starts a new game of Tic Tac Toe and alternates between the human player and the AI player until the game is over. The function prompts the human player to input their move, validates the input, and updates the game state accordingly. The AI player makes its move based on its learned strategy. The function displays the board after each move and prints out the final outcome of the game (win, loss, or tie)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1854aad5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-31T16:17:43.075328Z",
     "start_time": "2023-03-31T16:17:43.068533Z"
    }
   },
   "outputs": [],
   "source": [
    "from tictactoe import train, compete, play"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e07317b1",
   "metadata": {},
   "source": [
    "We will demonstrate the use of these functions in the next sections."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea757da4",
   "metadata": {},
   "source": [
    "## AI Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e82fe4fc",
   "metadata": {},
   "source": [
    "Let's now train our AI agent to play TicTacToe! We will train three different levels of AI according to the number of epochs (training simulations) that each agent will undergo.\n",
    "\n",
    "<img src='images/doggo.png' width=600/>\n",
    "\n",
    "We will save the corresponding policies on a folder so that we can use them when we play against our trained AI agents."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0a1a172",
   "metadata": {},
   "source": [
    "### Easy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d95af454",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-31T08:51:54.178056Z",
     "start_time": "2023-03-31T08:51:53.858402Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "epochs = 500\n",
    "policy_filepath_1 = './policies/player_1/easy.pickle'\n",
    "policy_filepath_2 = './policies/player_2/easy.pickle'\n",
    "\n",
    "train(policy_filepath_1, policy_filepath_2, epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0ec6eab",
   "metadata": {},
   "source": [
    "### Medium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "540d7332",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-31T08:52:02.073163Z",
     "start_time": "2023-03-31T08:52:01.371003Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "epochs = 1_000\n",
    "policy_filepath_1 = './policies/player_1/med.pickle'\n",
    "policy_filepath_2 = './policies/player_2/med.pickle'\n",
    "\n",
    "train(policy_filepath_1, policy_filepath_2, epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "975d6479",
   "metadata": {},
   "source": [
    "### Hard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4931fdb3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-31T08:52:12.960000Z",
     "start_time": "2023-03-31T08:52:06.202669Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "epochs = 10_000\n",
    "policy_filepath_1 = './policies/player_1/hard.pickle'\n",
    "policy_filepath_2 = './policies/player_2/hard.pickle'\n",
    "\n",
    "train(policy_filepath_1, policy_filepath_2, epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfa44525",
   "metadata": {},
   "source": [
    "## Competing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6feb05de",
   "metadata": {},
   "source": [
    "Now that we have trained our AI models. We can load this policies and test how each of the trained agent fare against each other.\n",
    "\n",
    "<img src='images/doggo_vs.png' width=450/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f313989",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-31T09:15:18.024775Z",
     "start_time": "2023-03-31T09:15:17.688123Z"
    }
   },
   "outputs": [],
   "source": [
    "games = 500\n",
    "policy_filepath_1 = './policies/player_1/hard.pickle'\n",
    "policy_filepath_2 = './policies/player_2/med.pickle'\n",
    "\n",
    "compete(policy_filepath_1, policy_filepath_2, games)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "061cd17a",
   "metadata": {},
   "source": [
    "## Playtime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2674534",
   "metadata": {},
   "source": [
    "It's time to play against our AI agents!\n",
    "\n",
    "Use the following cells to play against our trained AI. You may change the difficulty of the AI agent that you will play against by changing the policy filepath accordingly.\n",
    "\n",
    "Use `easy.pickle` for easy mode, `med.pickle` for medium difficulty, and `hard.pickle` for hard mode."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c38aadde",
   "metadata": {},
   "source": [
    "### Play as Player 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fa1ef1d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-31T09:04:47.333802Z",
     "start_time": "2023-03-31T09:01:49.430647Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "policy_filepath = './policies/player_2/hard.pickle'\n",
    "\n",
    "play(policy_filepath, first=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8263c209",
   "metadata": {},
   "source": [
    "### Play as Player 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "196b4807",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-31T09:01:43.755554Z",
     "start_time": "2023-03-31T08:59:08.236512Z"
    }
   },
   "outputs": [],
   "source": [
    "policy_filepath = './policies/player_1/hard.pickle'\n",
    "\n",
    "play(policy_filepath, first=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f7c50b0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-31T08:49:15.115597Z",
     "start_time": "2023-03-31T08:49:15.115591Z"
    }
   },
   "source": [
    "## Guide Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51e2ccd4",
   "metadata": {},
   "source": [
    "1. ***AI Agent Gameplay*** Try playing a few games with each AI agent using varying levels of difficulty. What can you observe with the way the AI agents play TicTacToe? Where you able to win at the hardest setting of the AI agent?\n",
    "2. ***Best First Move*** Based on how the AI agents play TicTacToe, what is the best first move when you are playing as the first player? How about the second player? *Hint: use the code found in the cell below to help you answer this question*\n",
    "3. ***Greedy Player*** Suppose the reinforcement learning player was *greedy*, meaning, it always played the moved that brought it to the position that it rated best without performing any *exploration* moves. Might it learn to play better, or worse, than a non-greedy player? What problems might occur?\n",
    "4. ***Other Improvements*** Can you think of other ways to improve the reinforcement learning player? Can you think of any better way to solve the tic-tac-toe problem as posed?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8d6566e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-31T16:01:41.627734Z",
     "start_time": "2023-03-31T16:01:41.616003Z"
    }
   },
   "outputs": [],
   "source": [
    "policy_filepath = './policies/player_1/hard.pickle'\n",
    "\n",
    "player = Player()\n",
    "player.loadPolicy(policy_filepath)\n",
    "policy = player.estimations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0933fed6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-31T16:01:43.937525Z",
     "start_time": "2023-03-31T16:01:43.907615Z"
    }
   },
   "outputs": [],
   "source": [
    "policy"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
